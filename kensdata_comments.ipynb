{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a1801c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\n",
    "from PIL import Image\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3000c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fca05373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_negative(v,props=''):\n",
    "    \"\"\"Style negative values\"\"\"\n",
    "    try:\n",
    "        return props if v < 0 else None\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def style_positive(v,props=''):\n",
    "    \"\"\"Style positive values\"\"\"\n",
    "    try:\n",
    "        return props if v > 0 else None\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def audience_simple(country):\n",
    "    '''SHOW TOP COUNTRIES'''\n",
    "    if country == \"US\":\n",
    "        return 'USA'\n",
    "    elif country == 'IN':\n",
    "        return 'INDIA'\n",
    "    else:\n",
    "        return 'Others'\n",
    "def find_names(m):\n",
    "    try:\n",
    "        return Aggremetvid2.loc[m,'Video title']\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def polarity(reviews):\n",
    "    return TextBlob(reviews).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "271bb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@st.cache\n",
    "def load_data():\n",
    "    Agg_met_country_suscriber =pd.read_csv('C:/Users/Bumblebee/Downloads/nlp-getting-started/kaggle files/youtube folder/Aggregated_Metrics_By_Country_And_Subscriber_Status.csv').iloc[1:,:]\n",
    "    Agg_met_video =pd.read_csv('C:/Users/Bumblebee/Downloads/nlp-getting-started/kaggle files/youtube folder/Aggregated_Metrics_By_Video.csv')\n",
    "    All_comments =pd.read_csv('C:/Users/Bumblebee/Downloads/nlp-getting-started/kaggle files/youtube folder/All_Comments_Final.csv')\n",
    "    video_performace =pd.read_csv('C:/Users/Bumblebee/Downloads/nlp-getting-started/kaggle files/youtube folder/Video_Performance_Over_Time.csv')\n",
    "    Agg_met_video.columns =[\"Video\",'Video title','Video publish time','comments added',\n",
    "                               'Shares','Dislikes','Likes','Suscribers lost','Suscribers gained',\n",
    "                               'RPM(USD)','CMP(USD)','Average percent viewed (%)','Average view duration',                                                                 \n",
    "                                'Views', 'Watch time (hours)','Subscribers' ,'Your estimated revenue (USD)',\n",
    "                                'Impressions','Impressions click throughrate(%)']\n",
    "\n",
    "    Agg_met_video['Video publish time'] = pd.to_datetime(Agg_met_video['Video publish time'])\n",
    "    Agg_met_video['Average view duration'] = Agg_met_video['Average view duration'].apply(lambda x:datetime.strptime(x,\"%H:%M:%S\"))\n",
    "    Agg_met_video['Average duration sec'] = Agg_met_video['Average view duration'].apply(lambda x:x.second + x.minute*60 +x.hour*3600)\n",
    "    Agg_met_video['Engagement_ratio'] = (Agg_met_video['comments added'] +Agg_met_video['Shares']+ Agg_met_video['Dislikes'] + Agg_met_video['Dislikes'] +Agg_met_video['Likes'])/Agg_met_video.Views\n",
    "    Agg_met_video['Views/sub gained'] = Agg_met_video['Views']/Agg_met_video['Suscribers gained']\n",
    "    Agg_met_video.sort_values('Video publish time',ascending = False,inplace = True)\n",
    "    video_performace['Date'] = pd.to_datetime(video_performace['Date'])\n",
    "    Aggremetvid2 = pd.read_csv('C:/Users/Bumblebee/Downloads/nlp-getting-started/kaggle files/youtube folder/Aggregated_Metrics_By_Video - Copy.csv',index_col ='Video')\n",
    "    comments = All_comments[['VidId','Comments']]\n",
    "    return  Agg_met_country_suscriber,Agg_met_video,All_comments,video_performace,Aggremetvid2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2941a1c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalHashError",
     "evalue": "module '__main__' has no attribute '__file__'\n\nWhile caching the body of `load_data()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function load_data at 0x000001D6040D48B0>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;31m# Hash the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"%s:%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file_should_be_hashed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_file_should_be_hashed\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    409\u001b[0m         return file_util.file_is_in_folder_glob(\n\u001b[1;32m--> 410\u001b[1;33m             \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_main_script_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m         ) or file_util.file_in_pythonpath(filepath)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_get_main_script_directory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[1;31m# script path in ScriptRunner.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mmain_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module '__main__' has no attribute '__file__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalHashError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BUMBLE~1\\AppData\\Local\\Temp/ipykernel_2784/2551504331.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mAgg_met_country_suscriber\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAgg_met_video\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAll_comments\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvideo_performace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mAggremetvid2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\caching.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshow_spinner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspinner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 573\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mget_or_create_cached_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    574\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mget_or_create_cached_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\caching.py\u001b[0m in \u001b[0;36mget_or_create_cached_value\u001b[1;34m()\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[1;31m# If we generated the key earlier we would only hash those\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m                 \u001b[1;31m# globals by name, and miss changes in their code or value.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m                 \u001b[0mcache_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_hash_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhash_funcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             \u001b[1;31m# First, get the cache that's attached to this function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\caching.py\u001b[0m in \u001b[0;36m_hash_func\u001b[1;34m(func, hash_funcs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;31m# because this step will be hashing any objects referenced in the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[1;31m# body.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m     update_hash(\n\u001b[0m\u001b[0;32m    625\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    626\u001b[0m         \u001b[0mhasher\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc_hasher\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36mupdate_hash\u001b[1;34m(val, hasher, hash_reason, hash_source, context, hash_funcs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_CodeHasher\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhash_funcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m     \u001b[0mch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhasher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, hasher, obj, context)\u001b[0m\n\u001b[0;32m    391\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mContext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[1;34m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m         \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mInternalHashError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36mto_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;31m# Hash the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 368\u001b[1;33m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb\"%s:%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_to_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m             \u001b[1;31m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mcode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__code__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_file_should_be_hashed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mco_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m                 \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m                 \u001b[0mdefaults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__defaults__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_file_should_be_hashed\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    408\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m         return file_util.file_is_in_folder_glob(\n\u001b[1;32m--> 410\u001b[1;33m             \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_main_script_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m         ) or file_util.file_in_pythonpath(filepath)\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\streamlit\\legacy_caching\\hashing.py\u001b[0m in \u001b[0;36m_get_main_script_directory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    719\u001b[0m         \u001b[1;31m# This works because we set __main__.__file__ to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[1;31m# script path in ScriptRunner.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mmain_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__main__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalHashError\u001b[0m: module '__main__' has no attribute '__file__'\n\nWhile caching the body of `load_data()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function load_data at 0x000001D6040D48B0>\n```\n\nPlease see the `hash_funcs` [documentation]\n(https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            "
     ]
    }
   ],
   "source": [
    "#loading data into streamlit\n",
    "Agg_met_country_suscriber,Agg_met_video,All_comments,video_performace,Aggremetvid2 = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e2f5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BUMBLE~1\\AppData\\Local\\Temp/ipykernel_2784/1077893190.py:3: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  median_agg =Agg_met_video2 [Agg_met_video2 ['Video publish time']>=metric_data_12mo].median()\n",
      "C:\\Users\\BUMBLE~1\\AppData\\Local\\Temp/ipykernel_2784/1077893190.py:3: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  median_agg =Agg_met_video2 [Agg_met_video2 ['Video publish time']>=metric_data_12mo].median()\n"
     ]
    }
   ],
   "source": [
    "Agg_met_video2 = Agg_met_video.copy()\n",
    "metric_data_12mo = Agg_met_video2['Video publish time'].max( ) - pd.DateOffset(months = 12)\n",
    "median_agg =Agg_met_video2 [Agg_met_video2 ['Video publish time']>=metric_data_12mo].median()\n",
    "numeric_cols = np.array((Agg_met_video2.dtypes ==\"float64\")|(Agg_met_video2.dtypes == 'int64'))\n",
    "Agg_met_video2.iloc[:,numeric_cols] = (Agg_met_video2.iloc[:,numeric_cols] - median_agg).div(median_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b025fca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a31d2f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_sidebar = st.sidebar.selectbox('Aggregate or individual video or Comments',('Aggregate Metrics','Individual video Analysis','Comments Analysis'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3ccaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4920ed30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\BUMBLE~1\\AppData\\Local\\Temp/ipykernel_2784/3428802047.py:8: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  metric_medians6mo = df_agg_metrics[df_agg_metrics['Video publish time']>=metric_date_6mo].median()\n",
      "C:\\Users\\BUMBLE~1\\AppData\\Local\\Temp/ipykernel_2784/3428802047.py:9: FutureWarning: DataFrame.mean and DataFrame.median with numeric_only=None will include datetime64 and datetime64tz columns in a future version.\n",
      "  metric_medians12mo = df_agg_metrics[df_agg_metrics['Video publish time']>=metric_date_12mo].median()\n",
      "C:\\Users\\BUMBLE~1\\AppData\\Local\\Temp/ipykernel_2784/3428802047.py:25: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  agg_numeric_lst = Agg_met_video2_final.median().index.tolist()\n"
     ]
    }
   ],
   "source": [
    "##Total Picture\n",
    "if add_sidebar == 'Aggregate Metrics':\n",
    "    df_agg_metrics = Agg_met_video[['Views','Video publish time','Likes','Subscribers',\n",
    "                                 'Shares','comments added','Average duration sec','Average percent viewed (%)','RPM(USD)','Engagement_ratio',\n",
    "                                'Views/sub gained']]\n",
    "    metric_date_6mo = df_agg_metrics['Video publish time'].max() -pd.DateOffset(months = 6)\n",
    "    metric_date_12mo = df_agg_metrics['Video publish time'].max() - pd.DateOffset(months =12)\n",
    "    metric_medians6mo = df_agg_metrics[df_agg_metrics['Video publish time']>=metric_date_6mo].median()\n",
    "    metric_medians12mo = df_agg_metrics[df_agg_metrics['Video publish time']>=metric_date_12mo].median()\n",
    "\n",
    "    col1,col2,col3,col4,col5 = st.columns(5)\n",
    "    columns = [col1,col2,col3,col4,col5]\n",
    "    count=0\n",
    "    for i in metric_medians6mo.index:\n",
    "        with columns[count]:\n",
    "            delta = (metric_medians6mo[i] -metric_medians12mo[i])/metric_medians12mo[i]\n",
    "            st.metric(label =i,value = round(metric_medians6mo[i],1),delta = '{:.2%}'.format(delta))\n",
    "            count +=1\n",
    "            if count >=5:\n",
    "                count =0\n",
    "    Agg_met_video2['publish date'] = Agg_met_video2['Video publish time'].apply(lambda x: x.date())\n",
    "    Agg_met_video2_final =Agg_met_video2.loc[:,['Video title','Views','publish date','Likes','Subscribers',\n",
    "                                 'Shares','comments added','Average duration sec','Average percent viewed (%)','RPM(USD)','Engagement_ratio',\n",
    "                                'Views/sub gained']]\n",
    "    agg_numeric_lst = Agg_met_video2_final.median().index.tolist()\n",
    "    add_to_pct = {}\n",
    "    for i in agg_numeric_lst:\n",
    "        add_to_pct[i] = '{:.1%}' .format \n",
    "    st.dataframe(Agg_met_video2_final.style.applymap(style_negative,props = 'color:red;').applymap(style_positive,props = 'color:green;').format(add_to_pct))\n",
    "   \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdde1641",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_20=lambda x:np.percentile(x,20)\n",
    "pct_20.__name__='pct_20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71f7bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge daily data with published data to get delta\n",
    "df_time_diff = pd.merge(video_performace,Agg_met_video.loc[:,['Video','Video publish time']],\n",
    "                     left_on = \"External Video ID\",right_on = 'Video')\n",
    "df_time_diff['days_published'] = (df_time_diff['Date'] - df_time_diff['Video publish time']).dt.days\n",
    "#collecting daily published data for 12 months\n",
    "date_12mo = Agg_met_video['Video publish time'].max() - pd.DateOffset(months = 12)\n",
    "df_time_diff_yr = df_time_diff[df_time_diff['Video publish time']>= date_12mo]\n",
    "\n",
    "#get daily view data (first 30) median & percentiles\n",
    "views_days = pd.pivot_table(df_time_diff_yr,index='days_published',values='Views',aggfunc=[np.mean,np.median,lambda x:np.percentile(x,80),pct_20]).reset_index()\n",
    "views_days.columns=['days_published','mean_views','median_views','80pct_views','20pct_views']\n",
    "views_days = views_days[views_days['days_published'].between(0,30)]\n",
    "views_cumulative = views_days.loc[:,['days_published','median_views','80pct_views','20pct_views']]\n",
    "views_cumulative.loc[:,['median_views','80pct_views','20pct_views']] = views_cumulative.loc[:,['median_views','80pct_views','20pct_views']].cumsum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a1524f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d8c21da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_sidebar == 'Individual video Analysis':\n",
    "    #selecting videos for selectbox\n",
    "    Videos = tuple(Agg_met_video['Video title'])\n",
    "    Videos_select = st.selectbox('pick A Video',Videos)\n",
    "    gg_filtered = Agg_met_video[Agg_met_video['Video title']== Videos_select]\n",
    "    agg_sub_filtered = Agg_met_country_suscriber[Agg_met_country_suscriber['Video Title']==   Videos_select]\n",
    "    agg_sub_filtered['Country'] = agg_sub_filtered['Country Code'].apply(audience_simple)\n",
    "    #ploting of number suscribers by location \n",
    "    agg_sub_filtered.sort_values('Is Subscribed',inplace= True)\n",
    "    fig = px.bar(agg_sub_filtered,x = 'Views',y='Is Subscribed', color ='Country',orientation = 'h')\n",
    "    st.plotly_chart(fig)\n",
    "    #ploting of percentile plot\n",
    "    agg_time_filtered =  df_time_diff[df_time_diff['Video Title'] ==  Videos_select]\n",
    "    first_30 = agg_time_filtered[agg_time_filtered['days_published'].between(0,30)]\n",
    "    first_30 = first_30.sort_values('days_published')\n",
    "    fig2 = go.Figure()\n",
    "    fig2.add_trace(go.Scatter(x=views_cumulative['days_published'],y=views_cumulative['20pct_views'],\n",
    "                              mode='lines',\n",
    "                              name= '20th percentile',line=dict(color='purple',dash = 'dash')))\n",
    "    fig2.add_trace(go.Scatter(x=views_cumulative['days_published'],y=views_cumulative['median_views'],\n",
    "                              mode='lines',\n",
    "                              name= '50th percentile',line=dict(color='black',dash = 'dash')))\n",
    "    fig2.add_trace(go.Scatter(x=views_cumulative['days_published'],y=views_cumulative['80pct_views'],\n",
    "                              mode='lines',\n",
    "                              name= '80th percentile',line=dict(color='royalblue',dash = 'dash')))\n",
    "    fig2.add_trace(go.Scatter(x=first_30['days_published'],y=first_30['Views'].cumsum(),\n",
    "                              mode='lines',\n",
    "                              name= 'current video',line=dict(color='firebrick',width=8)))    \n",
    "    fig2.update_layout(title= 'view comparison first 30 days',xaxis_title = 'days since published',yaxis_title='views commulative')\n",
    "    st.plotly_chart(fig2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43b43cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collecting comments data\n",
    "comments = All_comments[['VidId','Comments']]\n",
    "a=comments['Comments'].apply(lambda x:re.findall(r'[^.].*'+'[?$]',str(x)))#extracting questions\n",
    "b=comments[\"VidId\"].apply(lambda x:find_names(x))#matching video_id to corresponding Video names\n",
    "#turning data to dataframes(a and b)\n",
    "a=pd.DataFrame(a)\n",
    "b=pd.DataFrame(b)\n",
    "comments=pd.concat([comments,a,b], axis=1, ignore_index=True)#joining a&b to comments dataframe\n",
    "comments=comments.rename(columns={0:\"VidId\",1:'Comments',2:'Questions',3:'Video Title'})#renaming title\n",
    "comments=comments.reindex(columns=[\"VidId\",'Video Title','Comments','Questions'])#renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if add_sidebar=='Comments Analysis':\n",
    "    #selecting video names for select bar\n",
    "    Video_slect=comments['Video Title'].value_counts()\n",
    "    Videos = tuple(Video_slect.index)\n",
    "    Videos_select = st.selectbox('pick A Video',Videos)\n",
    "    comm_video_names= comments[comments['Video Title']==   Videos_select]\n",
    "    #select questions asked for each video \n",
    "    for i in comm_video_names['Video Title']:\n",
    "        filt= (comments['Video Title'] == i)\n",
    "        a=comments.loc[filt,['Comments','Questions']]\n",
    "    st.dataframe(a)\n",
    "    #running a sentiment analysis for comment section\n",
    "    comm_video_names['Comments']=comm_video_names['Comments'].astype('str')\n",
    "    comm_video_names['polarity'] = comm_video_names['Comments'].apply(lambda x :polarity(x))\n",
    "    comm_video_names['Expression']=np.where(comm_video_names['polarity'] > 0,\"Positive\",'Negative')\n",
    "    comm_video_names.loc[comm_video_names.polarity ==0,'Expression']= 'Neutral'\n",
    "    #ploting a countplot to show sentiment for each video\n",
    "    siz =(3,2)\n",
    "    fig,ax = plt.subplots(figsize= siz,dpi=40)\n",
    "    ax=sns.countplot(x = 'Expression',data=comm_video_names,palette = 'Set1')\n",
    "\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    #plotting wordcloud for each selected video\n",
    "    for i in comm_video_names['Video Title']:\n",
    "        filt= (comments['Video Title'] == i)\n",
    "        a=comments.loc[filt,['Comments']].values\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wc = WordCloud(stopwords = stopwords).generate(str(a))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter([1, 2, 3], [1, 2, 3])\n",
    "    plt.imshow(wc,interpolation = 'bilinear' )\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    st.pyplot(fig,figsize= siz,dpi=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c393ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc3947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c683c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e826e548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "b7aba269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7206c72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8c21f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9ec4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
